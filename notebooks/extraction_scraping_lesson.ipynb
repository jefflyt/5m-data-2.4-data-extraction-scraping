{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a real-world example to demonstrate the concepts of data extraction via REST API and GraphQL. The APIs we will be using are the [GitHub REST API](https://docs.github.com/en/rest) and [GitHub GraphQL API](https://docs.github.com/en/graphql).\n",
    "\n",
    "They are both APIs provided by GitHub to access data on their platform. The GraphQL API is a newer API. You can access data such as user profiles, repositories, issues, pull requests, releases etc. from GitHub.\n",
    "\n",
    "We will use the following libraries for this lesson:\n",
    "\n",
    "- `requests` for making HTTP requests\n",
    "- `json` for parsing JSON responses\n",
    "- `pandas` and `numpy` for data manipulation\n",
    "- `sqlalchemy` for writing data to database (DuckDB)\n",
    "- `beautifulsoup4` for web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API\n",
    "\n",
    "Usually, before we can make a request to an API, we need to register for an API key. This is for the API provider to track usage, prevent abuse and to authenticate users. For more information on how to authenticate with GitHub API, refer to the [GitHub API docs](https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28#authentication). \n",
    "\n",
    "For this lesson, you will need to a personal access token (API key). If you do not have an API key, you can create a new one by folllowing the instructions in the official GitHub documentation [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token).\n",
    "\n",
    "We will be retrieving data from some popular repositories on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please input your GitHub personal access token here\n",
    "access_token = '<YOUR-GITHUB-ACCESS-TOKEN>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github API endpoints are in the format \n",
    "```\n",
    "https://api.github.com/repos/{owner}/{repo}\n",
    "```\n",
    "and appended with the resource you want to access.\n",
    "\n",
    "For example, to access the `issues` resource, the endpoint is\n",
    "```\n",
    "https://api.github.com/repos/{owner}/{repo}/issues\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Here are some common endpoints:\n",
    "\n",
    "| Resource | Endpoint |\n",
    "| --- | --- |\n",
    "| Issues | `/issues` |\n",
    "| Pull Requests | `/pulls` |\n",
    "| Commits | `/commits` |\n",
    "| Contributors | `/contributors` |\n",
    "| Languages | `/languages` |\n",
    "| Releases | `/releases` |\n",
    "| Tags | `/tags` |\n",
    "| Branches | `/branches` |\n",
    "| Forks | `/forks` |\n",
    "| Stargazers | `/stargazers` |\n",
    "| Subscribers | `/subscribers` |\n",
    "| Subscription | `/subscription` |\n",
    "\n",
    "Here are the HTTP verbs for the Github API:\n",
    "\n",
    "| Verb | Description |\n",
    "| --- | --- |\n",
    "| GET | Used for retrieving resources. |\n",
    "| POST | Used for creating resources. |\n",
    "| PATCH | Used for updating resources with partial JSON data. For instance, an Issue resource has title and body attributes. A PATCH request may accept one or more of the attributes to update the resource. |\n",
    "| PUT | Used for replacing resources or collections. For PUT requests with no body attribute, be sure to set the Content-Length header to zero. |\n",
    "| DELETE | Used for deleting resources. |\n",
    "\n",
    "We will only be using the GET verb for this project, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` is an open-source library, its code repository is hosted on GitHub. You can access the repository at [https://github.com/pandas-dev/pandas](https://github.com/pandas-dev/pandas).\n",
    "\n",
    "Let's try to extract the past `releases` (library versions) from the the repository. The API endpoint is `https://api.github.com/repos/pandas-dev/pandas/releases`.\n",
    "\n",
    "The access token needs to be passed in the `Authorization` header. The token is a personal access token (PAT) which you can generate on GitHub. The token is used to authenticate you as a user and to authorize you to access the repository.\n",
    "\n",
    "It is also recommended to pass in the following headers:\n",
    "\n",
    "- `Accept: application/vnd.github+json` - to specify the preferred format of the response\n",
    "- `X-GitHub-Api-Version:2022-11-28` - to specify the version of the API to use\n",
    "\n",
    "---\n",
    "\n",
    "If we use a `GET` request to access the endpoint, we will get a list of releases in JSON format (default limit of _30 results_ per page). More info on the `releases` resource [here](https://docs.github.com/en/rest/releases/releases?apiVersion=2022-11-28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/pandas-dev/pandas/releases\", \n",
    "                        headers={\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the response is successful by checking the status code of the response. If the status code is 200, then the response is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.json()` method to return the JSON format of the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1 = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(releases_page_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(releases_page_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1[0]['published_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latest version of `pandas` is the one shown above. Hence, the list is sorted from most recent to oldest.\n",
    "\n",
    "While the last release on the list is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1[-1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1[-1]['published_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the default results per page is `30`. You can increase the number of results per page to a maximum of `100` by using the `per_page` query parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/pandas-dev/pandas/releases?per_page=100\", \n",
    "                        headers={\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1_100 = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(releases_page_1_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_1_100[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default page is `1`, you can get the 2nd page by passing the `page` query parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/pandas-dev/pandas/releases?per_page=100&page=2\", \n",
    "                        headers={\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_page_2_100 = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(releases_page_2_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no 2nd page as the total no of releases is only 98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We're interested in the following fields and want to save them into a dataframe:\n",
    "\n",
    "- `tag_name`: The version name/number of the release.\n",
    "- `published_at`: The date and time when the release was published.\n",
    "- `body`: The release notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases_with_essential_fields = [{\"version\": release[\"tag_name\"], \"published_at\": release[\"published_at\"], \"summary\": release[\"body\"]} for release in releases_page_1_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_releases = pd.DataFrame(releases_with_essential_fields)\n",
    "\n",
    "pandas_releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert `published_at` to the correct datetime format\n",
    "pandas_releases['published_at'] = pd.to_datetime(pandas_releases['published_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the average interval (duration or differences in time) for pandas' releases (in no of days)?\n",
    ">\n",
    "> Hint 1: Sort the dataframe by `published_at` in ascending order.\n",
    "> \n",
    "> Hint 2: Use [diff()](https://pandas.pydata.org/docs/reference/api/pandas.Series.diff.html) to calculate the time deltas between each `published_at`.\n",
    ">\n",
    "> Refer to [Time deltas](https://pandas.pydata.org/pandas-docs/stable/user_guide/timedeltas.html#time-deltas) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Print the `summary` of the first (1.0.0) and second (2.0.0) major releases of pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the dataframe to a table in DuckDB.\n",
    "\n",
    "First, we need to import the required libraries and get the absolute path of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla\n",
    "import os \n",
    "\n",
    "parent_dir = os.path.abspath(os.path.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqla.create_engine(f'duckdb:///{parent_dir}/output/unit-2-3.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_releases.to_sql(\"pandas_releases\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use another resource (endpoint) now. There are many active contributors to the pandas library, and suppose we want to know how many commits each contributor has contributed. We can retrieve the data using the `stats/contributors` endpoint.\n",
    "\n",
    "More info on the endpoint can be found [here](https://docs.github.com/en/rest/metrics/statistics?apiVersion=2022-11-28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/pandas-dev/pandas/stats/contributors\", \n",
    "                        headers={\"Accept\": \"application/vnd.github+json\", \"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors[0][\"author\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get the `total` and `login` (under `author`) fields and insert them into a dataframe. Then find out who has the most number of commits.\n",
    ">\n",
    "> Then, write the dataframe into a table named `pandas_contributors_commits` in duckdb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL\n",
    "\n",
    "Github offers a new way to access its data, which is the GraphQL API. You can read more about it here: https://docs.github.com/graphql\n",
    "\n",
    "GraphQL is introspetive, which means you can query the schema of the API itself. This is very useful when you are trying to figure out what data you can access. You can access the schema via the GraphQL API Explorer: https://docs.github.com/en/graphql/overview/explorer and run the following query:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  __schema {\n",
    "    types {\n",
    "      name\n",
    "      kind\n",
    "      description\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Or to get the fields of a specific type (for example `\"Release\"`), you can run the following query:\n",
    "\n",
    "```graphql\n",
    "{\n",
    "  __type(name: \"Release\") {\n",
    "    name\n",
    "    kind\n",
    "    description\n",
    "    fields {\n",
    "      name\n",
    "      description\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "It should return the following results (in JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"data\": {\n",
    "    \"__type\": {\n",
    "      \"name\": \"Release\",\n",
    "      \"kind\": \"OBJECT\",\n",
    "      \"description\": \"A release contains the content for a release.\",\n",
    "      \"fields\": [\n",
    "        {\n",
    "          \"name\": \"author\",\n",
    "          \"description\": \"The author of the release\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"createdAt\",\n",
    "          \"description\": \"Identifies the date and time when the object was created.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"databaseId\",\n",
    "          \"description\": \"Identifies the primary key from the database.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"description\",\n",
    "          \"description\": \"The description of the release.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"descriptionHTML\",\n",
    "          \"description\": \"The description of this release rendered to HTML.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"id\",\n",
    "          \"description\": null\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"isDraft\",\n",
    "          \"description\": \"Whether or not the release is a draft\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"isLatest\",\n",
    "          \"description\": \"Whether or not the release is the latest releast\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"isPrerelease\",\n",
    "          \"description\": \"Whether or not the release is a prerelease\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"mentions\",\n",
    "          \"description\": \"A list of users mentioned in the release description\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"name\",\n",
    "          \"description\": \"The title of the release.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"publishedAt\",\n",
    "          \"description\": \"Identifies the date and time when the release was created.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"reactionGroups\",\n",
    "          \"description\": \"A list of reactions grouped by content left on the subject.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"reactions\",\n",
    "          \"description\": \"A list of Reactions left on the Issue.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"releaseAssets\",\n",
    "          \"description\": \"List of releases assets which are dependent on this release.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"repository\",\n",
    "          \"description\": \"The repository that the release belongs to.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"resourcePath\",\n",
    "          \"description\": \"The HTTP path for this issue\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"shortDescriptionHTML\",\n",
    "          \"description\": \"A description of the release, rendered to HTML without any links in it.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"tag\",\n",
    "          \"description\": \"The Git tag the release points to\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"tagCommit\",\n",
    "          \"description\": \"The tag commit for this release.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"tagName\",\n",
    "          \"description\": \"The name of the release's Git tag\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"updatedAt\",\n",
    "          \"description\": \"Identifies the date and time when the object was last updated.\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"url\",\n",
    "          \"description\": \"The HTTP URL for this issue\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"viewerCanReact\",\n",
    "          \"description\": \"Can user react to this subject\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Follow the guide [here](https://docs.github.com/en/graphql/guides/introduction-to-graphql#discovering-the-graphql-api) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the schema via a `GET` request to the endpoint `https://api.github.com/graphql`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/graphql\", \n",
    "                        headers={\"Authorization\": f\"Bearer {access_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas['data']['__schema'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(schemas['data']['__schema']['types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas['data']['__schema']['types'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the same result as the `__schema` query above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GraphQL API only has a single endpoint:\n",
    "\n",
    "```\n",
    "https://api.github.com/graphql\n",
    "```\n",
    "\n",
    "In REST, HTTP verbs determine the operation performed. In GraphQL, you'll provide a JSON-encoded body whether you're performing a `query` or a `mutation`, so the HTTP verb is `POST`. The exception is an introspection query, which is a simple `GET` to the endpoint (which we did above).\n",
    "\n",
    "To query GraphQL via `requests`, make a POST request with a JSON payload. The payload must contain a string called `query`.\n",
    "\n",
    "GraphQL queries return only the data you specify. To form a query, you must specify fields within fields (also known as nested subfields) until you return only scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's replicate the REST API exercise above where we extract the `releases`. You can see the resources in GraphQL which you can access [here](https://docs.github.com/en/graphql/reference/objects) - search for `releases`. \n",
    "\n",
    "We must specify the fields we want to extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "query {\n",
    "    repository(owner: \"pandas-dev\", name: \"pandas\") {\n",
    "        releases(first: 100) {\n",
    "            totalCount\n",
    "            edges {\n",
    "                node {\n",
    "                    tagName\n",
    "                    description\n",
    "                    publishedAt\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "            \n",
    "\n",
    "response = requests.post(\"https://api.github.com/graphql\", \n",
    "                        headers={\"Authorization\": f\"Bearer {access_token}\"}, json={\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the composition line by line:\n",
    "\n",
    "`query {`\n",
    "\n",
    "Because we want to read data from the server, not modify it, query is the root operation. (If you don't specify an operation, query is also the default.)\n",
    "\n",
    "`repository(owner:\"pandas-dev\", name:\"pandas\") {`\n",
    "\n",
    "To begin the query, we want to find a repository object. The schema validation indicates this object requires an owner and a name argument.\n",
    "\n",
    "`releases(first: 100) {`\n",
    "\n",
    "To account for all releases in the repository, we call the releases object. (We could query a single release on a repository, but that would require us to know the tagName of the release we want to return and provide it as an argument.)\n",
    "\n",
    "Some details about the releases object:\n",
    "\n",
    "The docs tell us this object has the type `ReleaseConnection`. Schema validation indicates this object requires a last or first number of results as an argument, so we provide first 100. You can find more information about the `ReleaseConnection` type [here](https://docs.github.com/en/graphql/reference/objects#releaseconnection).\n",
    "\n",
    "- `totalCount`\n",
    "\n",
    "    The beauty of GraphQL is that we can retrieve the totalCount of the releases object by simply adding it to the query and it will be returned.\n",
    "\n",
    "- `edges {`\n",
    "\n",
    "    We know releases is a connection because it has the ReleaseConnection type. To retrieve data about individual release, we have to access the node via edges.\n",
    "\n",
    "- `nodes {`\n",
    "\n",
    "    Here we retrieve the nodes at the end of the edge. The ReleaseConnection docs indicate the nodes at the end of the ReleaseConnection type is a Release object.\n",
    "\n",
    "    Now that we know we're retrieving a Release object, we can look at the docs and specify the fields we want to return:\n",
    "    ```\n",
    "    tagName\n",
    "    description\n",
    "    publishedAt\n",
    "    ```\n",
    "\n",
    "    Here we specify the tagName, description, and publishedAt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(releases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases['data']['repository']['releases']['totalCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases['data']['repository']['releases']['edges'][0]['node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "releases['data']['repository']['releases']['edges'][-1]['node']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Query the `issues` resource, return the first 50 issues with the fields- `title`, `createdAt` and `author`, under the `author` field, return the `login` field.\n",
    ">\n",
    "> Refer to this [link](https://docs.github.com/en/graphql/reference/objects#issue) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.scrapethissite.com/pages/simple/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use BeautifulSoup to parse the HTML above. There are many parsers available. The default parser is `html.parser`, it does not require any additional installation. However, its performance is not the best. If you want better performance, you can use `lxml` parser. Refer to [here](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=get#installing-a-parser) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look for all paragraphs in the page using the `find_all` method. This returns a list of all the paragraph `Tag`s in the page.\n",
    "\n",
    "Refer to the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#bs4.Tag) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual text\n",
    "paragraphs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all the country names in the HTML by specifying the tag and class name. It is always good to be as specific as possible when selecting elements. This is because the more specific you are, the less likely you will select other unintended elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_elements = soup.find_all('h3', 'country-name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently\n",
    "\n",
    "`soup.find_all('h3', class_='country-name')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extract just the country names string into a new list called `country_names`. Remove all whitespaces.\n",
    ">\n",
    "> Your results should be starting with `['Andorra', 'United Arab Emirates', 'Afghanistan', 'Antigua and Barbuda', ...]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extract the capitals into a new list called `capital_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to collect all the required information at one go. For each country element, you can further use the `find` method to extract the child element.\n",
    "\n",
    "Let's collect the capital, population and area for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "for country in soup.find_all('div', 'col-md-4 country'):\n",
    "    country_info = {}\n",
    "    country_info['name'] = country.find('h3').text.strip()\n",
    "    country_info['capital'] = country.find('span', 'country-capital').text\n",
    "    country_info['population'] = country.find('span', 'country-population').text\n",
    "    country_info['area_km2'] = country.find('span', 'country-area').text\n",
    "    countries.append(country_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape a more complex example. This page has a table with pagination. We will scrape the rows in all the pages and store the data in a dataframe.\n",
    "\n",
    "[https://www.scrapethissite.com/pages/forms/](https://www.scrapethissite.com/pages/forms/)\n",
    "\n",
    "First, as with any web scraping task, we need to inspect the page to understand the structure of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by scraping the first page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.scrapethissite.com/pages/forms/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the table header first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = soup.find('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [th.text.strip() for th in header.find_all('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find all the rows on first page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = soup.find_all('tr', 'team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in teams[0].find_all('td'):\n",
    "    print(col.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine that with the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = {}\n",
    "for header, col in zip(headers, teams[0].find_all('td')):\n",
    "    row_dict[header] = col.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat the operation above for all the teams (rows) using a `for` loop, and append each row_dict to a list called `rows`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you press on any page number (button) in the bottom, you will see the URL change, for example pressing on page 2:\n",
    "\n",
    "`https://www.scrapethissite.com/pages/forms/?page_num=2`\n",
    "\n",
    "This is because the page number is a parameter in the URL. We can use this to our advantage to scrape all the pages. We can use a `for loop` to iterate through all the pages and scrape the data from each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_and_extract_rows(response_text: str):\n",
    "    \"\"\"\n",
    "    Parse the HTML and extract table rows from the response object's text.\n",
    "    \n",
    "    Args:\n",
    "        response_text (str): The HTML text from the response object.\n",
    "        \n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(response_text, \"html.parser\")\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 24 pages, we need to iterate through all of them to get all the data we want. We will use a for loop to do this. We will also use the `time.sleep()` function to pause and make sure we don't overload the server with requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for page in range(1, 5):\n",
    "    r = requests.get(f\"https://www.scrapethissite.com/pages/forms/?page_num={page}\")\n",
    "    for row_dict in parse_and_extract_rows(r.text):\n",
    "        rows.append(row_dict)\n",
    "    # pause for 1 second between requests\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the list of dictionaries into a dataframe to perform further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cast the correct dtypes for the columns, we'll use pandas [nullable](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes) integer (`pd.Int64Dtype`) and float (`pd.Float64Dtype`) type for the numeric columns.\n",
    "\n",
    " Below, `int` is an alias for `np.int64`, while `Int64` represents `pd.Int64Dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('', pd.NA).astype({'Year': int, 'Wins': 'Int64', 'Losses': 'Int64', 'OT Losses': 'Int64', \n",
    "                                   'Goals For (GF)': 'Int64', 'Goals Against (GA)': 'Int64', \n",
    "                                   '+ / -': 'Int64', 'Win %': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Find the team that has the most number of Wins for the year.\n",
    "> \n",
    "> 2. Find the team that has the most number of Losses for the year.\n",
    "> \n",
    "> 3. Find the team that has the lowest Win % for the year.\n",
    "> \n",
    "> 4. Find the team that has the most number of Wins for all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_extract_rows(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extract table rows from the parsed HTML.\n",
    "    \n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "        \n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "page = 1\n",
    "r = requests.get(f\"https://www.scrapethissite.com/pages/forms/?page_num={page}\")\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "for row_dict in parse_and_extract_rows(soup):\n",
    "    rows.append(row_dict)\n",
    "\n",
    "while soup.find(\"a\", {\"aria-label\": \"Next\"}):\n",
    "    page += 1\n",
    "    r = requests.get(f\"https://www.scrapethissite.com/pages/forms/?page_num={page}\")\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for row_dict in parse_and_extract_rows(soup):\n",
    "        rows.append(row_dict)\n",
    "    # pause for 1 second between requests\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
